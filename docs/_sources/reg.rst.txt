.. _regularization:

==============
Regularization
==============


In mathematics, statistics, and computer science, particularly in the fields of machine learning and inverse problems, regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting (`Wikipedia Regularization`_). 

Due to the sparsity within our data, our training sets will often be ill-posed (singular).  Applying regularization to the regression has many advantages, including:

1. Converting ill-posed problems to well-posed by adding additional information via the penalty parameter :math:`\lambda`
2. Preventing overfitting
3. Variable selection and the removal of correlated variables (`Glmnet Vignette`_).  The Ridge method shrinks the coefficients of correlated variables while the LASSO method picks one variable and discards the others.  The elastic net penalty is a mixture of these two; if variables are correlated in groups then :math:`\alpha=0.5` tends to select the groups as in or out. If Î± is close to 1, the elastic net performs much like the LASSO method and removes any degeneracies and wild behavior caused by extreme correlations. 





Ridge regression
++++++++++++++++

.. math::

	\min _{\beta\in \mathbb {R} ^{p}}{\frac {1}{n}}\|{\hat {X}}\beta-{\hat {Y}}\|^{2}+\lambda \|\beta\|_{2}^{2}


Least Absolute Shrinkage and Selection Operator (LASSO)
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

.. math::

	\min _{\beta\in \mathbb {R} ^{p}}{\frac {1}{n}}\|{\hat {X}}\beta-{\hat {Y}}\|^{2}+\lambda\|\beta\|_{1}


Elastic net
+++++++++++

.. math::

	\min _{\beta\in \mathbb {R} ^{p}}{\frac {1}{n}}\|{\hat {X}}\beta-{\hat {Y}}\|^{2}+\lambda (\alpha \|\beta\|_{1}+(1-\alpha )\|\beta\|_{2}^{2}),\alpha \in [0,1]


.. _Wikipedia Regularization: https://en.wikipedia.org/wiki/Regularization_(mathematics)

.. _Wikipedia Regularization: https://en.wikipedia.org/wiki/Regularization_(mathematics)
.. _Glmnet Vignette: https://en.wikipedia.org/wiki/Mean_squared_error


